\documentclass{bioinfo}
\copyrightyear{2012}
\pubyear{2012}

\begin{document}
\firstpage{1}

\title[istar]{istar: Software-as-a-Service Platform for idock and igrep}
\author[Hongjian Li \textit{et~al}]{Hongjian Li\footnote{to whom correspondence should be addressed}, Kwong-Sak Leung and Man-Hon Wong}
\address{Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, New Territories, Hong Kong, China}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\maketitle

\begin{abstract}
\section{Summary:}
We were motivated by the desire to automate large-scale protein-ligand docking using our idock and thus developed istar, a general SaaS (Software as a Service) platform. Without tedious software installation, users, especially computational chemists, can submit jobs on the fly either by browsing our web site or by programming against our RESTful API. Our HTML5- and CSS3-powered web site supports filtering and previewing ligands to dock, as well as monitoring job progress in real time, a very useful functionality commonly lacked in other SaaS platforms like DOCK BLASTER. We developed a customized daemon from idock 1.5, featuring our innovative idea of two-phase docking. We deployed several high-performance workstations to run the daemon in the background and exploited fine-grained slice-level parallelism in phase 1. The daemon compresses output using gzip to save server storage and network bandwidth.

\section{Availability:}
istar is free and open source under Apache License 2.0. Its C++ and JavaScript source code is available at https://github.com/HongjianLi/istar. A live demo is running at http://istar.cse.cuhk.edu.hk.

\section{Contact:} \href{hjli@cse.cuhk.edu.hk}{hjli@cse.cuhk.edu.hk}
\end{abstract}

\section{Introduction}

In 2011, we developed idock 1.0 \citep{1153}, a multithreaded virtual screening tool for flexible ligand docking. In 2012, we developed idock 1.5, further improving docking speed and accuracy, inventing new functionalities, and fixing bugs. Comprehensive benchmark showed that idock 1.5 displayed comparable success rates of redocking but outperformed state-of-the-art AutoDock Vina 1.1.2 \citep{595} in terms of docking speed by at least 8.69 times and at most 37.51 times across 12 receptors and 3000 ligands.

Having released idock, we kept receiving docking requirements from our colleagues and collaborators. They are mostly biochemists and pharmacists, outsourcing the docking research to us after discovering potential biological targets for certain diseases of therapeutic interest. All of a sudden, we had to grab the protein structure, do format conversion, define search space, set up docking parameters, and keep running idock in batch for months. Tedious enough, all the above work was done manually, resulting in very low research productivity.

Figure \ref{istar:architecture} shows the architecture of istar. Under typical circumstance, a user opens a browser and submits a job. The web server validates user input and saves it into database. Several workstations fetch jobs from the database and actually carry out the massive experiments. Upon completion, they send the user a completion notification email and write the result to a network file system, which is served as static content by the web server. The user again browses our web site to track progress and retrieve result.

\begin{figure}
\centerline{\includegraphics[width=\linewidth]{Architecture.png}}
\caption{istar architecture.}\label{istar:architecture}
\end{figure}

On the client side, we seamlessly combined both the Twitter Bootstrap and the HTML5 Boilerplate into a HTML5- and CSS3-powered web site, which was successfully checked as HTML5 by the W3C Markup Validator v1.3. We adopted the \textit{de facto} jQuery library to simplify HTML document traversing, event handling, animating, and Ajax interactions, and the jQuery UI library to provide themeable widgets. We tested the web site with Chrome 19+, Firefox 12+, Internet Explorer 9+, Safari 5+ and Opera 12+, and utilized the Modernizr library to detect HTML5 and CSS3 features in order to maintain backward compatibility in older browsers.

On the server side, we built the web server in the same JavaScript language using the well-known asynchronous event-driven node.js on top of the express server, forking multiple worker processes to accept simultaneous HTTP/SPDY connections. We utilized the spdy node module to support SPDY protocol v3, the prototype of next generation HTTP 2.0. We employed the forever node module to restart worker processes automatically in case of unhandled exceptions, ensuring failover and high availability. We made use of regular expressions and developed a customized version of the validator node module to validate and sanitize user input upon job receival, ensuring data validity.

On the database side, we chose MongoDB, a scalable, high-performance, open source NoSQL database. MongoDB features document store in JSON style, making it particularly suitable for applications requiring flexible document attributes like our istar. MongoDB is written in C++, making it easy to manipulate the database from idock and igrep, which are also written in C++. By using a 10gen-supported native MongoDB driver for node.js, the web server is able to save user input as jobs into MongoDB.

On the workstation side, we are maintaining a Linux workstation, equipped with Intel Xeon W3520 @ 2.66 GHz, 8GB DDR3 SDRAM and NVIDIA GeForce GTX 285 (1024 MB), and running a minimal version of Fedora 17 x64. Meanwhile, from our department we have exclusively reserved 4 Mac workstations, equipped with Intel Core i5-2400 @ 3.10GHz and 4GB DDR3 SDRAM, and running Mac OS X Lion 10.7.4 Build 11E53.

On the network file system side, we mounted a 2TB hard disk for shared use. All the 5 workstations can simultaneously access to the hard disk and perform file reading and writing. In principle, we will persist existing jobs and results until storage shortage.

\section{idock on istar}

On the client side, the upper section displays information about existing jobs, including the number of ligands to dock, submission date, current status and progress, and result for download. Client-side paging is supported, and an Ajax timer is started to automatically refresh the status and progress in real time by querying against the web server every second, a very useful functionality commonly lacked in other SaaS platforms like DOCK BLASTER \citep{557}. The lower section allows new job submission. A new job typically consists of several compulsory fields as well as several optional fields. Compulsory fields include a receptor in PDBQT format, a search space defined by a cubic box, a short job description, and an email to receive completion notification. Optional fields include 9 ligand filtering conditions, like molecular weight, partition coefficient xlogP, apolar desolvation, polar desolvation, number of hydrogen bond donors, number of hydrogen bond acceptors, topological polar surface area tPSA, net charge, and number of rotatable bonds. We set up default values for optional fields, and only the ligands satisfying all the 9 filtering conditions will be docked. Figure \ref{istar:idock-rest} shows the RESTful API of idock. We expose the functionalities of job submission, job query, and ligand counting as RESTful API for others to program against.

On the server side, during startup, the master process loads and parses 9 molecular properties of 12 million ligands (Figure \ref{istar:LigandProperties}), and awaits queries from worker processes through inter-process message passing. Upon receival of a query from a worker process, the master process performs an in-memory table scan and calculates the number of ligands satisfying a given combination of filtering conditions on those 9 molecular properties. The ligand count is signaled to the query-invoking worker process and then forwarded to the client side. The entire round trip costs approximately one second, rendering the feasibility of real-time ligand counting queries for users to estimate in advance how many ligands will be docked. Originally we stored the 9 molecular properties in MongoDB, but our in-house trial showed that it took about 9 seconds to complete a ligand counting query even in the presence of index and even having the data stored in a RAM disk. Disappointed by the poor performance of MongoDB given 12 million documents, we decided to abandon this design and let the master process hold the data in memory throughout its lifecycle.

On the network file system side, we collected 12,171,187 clean ligands at pH 7 in mol2 format from the fruitful ZINC database \citep{532,1178} with explicit permission of its major developer and maintainer, Prof. John J. Irwin at University of California, San Francisco. Originally the yuck-compound-free ligands are organized into 98 slices, with each slice containing approximately 125,000 clean ligands, among which about 75,000 (60\%) have a molecular weight of at least 350g/mol, the minimum weight requirement that a modern drug should bear. We converted all the 12 million ligands into PDBQT format in batch, and combined the 12 million individual files and integrated their 9 molecular properties into one single file as huge as 50GB for subsequent parallel docking. The format conversion alone required 2 months. We also recorded and encoded the offset of each ligand within that big file for fast random seeking.

On the workstation side, we developed a customized version of idock as a daemon in modern C++11, adding necessary code to iteratively fetch a pending job slice from MongoDB, reuse receptor and grid maps whenever possible, perform 2-phase docking, report progress to MongoDB, and send email notification upon job completion. We deliberately demoted double precision floating point to single precision and configured the grid map granularity to 0.1\AA\ in order to lower down memory consumption due to the 4GB RAM limit of the Mac workstations.

On one hand, the idock on istar features 2-phase docking (Figure \ref{istar:2PhaseDocking}), due to as many as 12 million ligands to screen. In phase 1, idock performs coarse but fast virtual screening without writing any conformations to file, aiming to quickly shortlist a few thousand candidate compounds. In phase 2, idock performs fine but slow virtual screening with a significantly larger number of Monte Carlo tasks per ligand, writing as many conformations to file as possible and aiming to refine the predicted free energy as well as predicted conformation of candidate compounds. Such a 2-phase docking methodology can remarkably reduce job execution time while avoiding the risk of filtering out potentially promising compounds, controlling the false negative rate at an acceptable level.

\begin{figure}
\centerline{\includegraphics[width=\linewidth]{2PhaseDocking.png}}
\caption{Two-phase docking.}\label{istar:2PhaseDocking}
\end{figure}

On the other hand, the idock on istar features slice-level parallelism in phase 1. Generally speaking, multiple workstations can compete for either jobs or slices, with the former known as job-level parallelism and the latter as slice-level parallelism. Job-level parallelism is very straightforward to implement and can ensure high utilization of computational power when the number of jobs exceeds the number of workstations. Nevertheless, when the number of workstations exceeds the number of jobs, which is usually the case in practice during the initial stage of istar, slice-level parallelism can better utilize computational power by subdividing a job into slices which are then distributed to workstations. Slice-level parallelism is, in contrast, difficult to implement on both the database side and the workstation side. The technical hurdle becomes even more apparent when results from multiple workstations must be properly combined to produce a final result and progresses from multiple workstations must be properly combined too to compute an overall progress. Finally we succeeded in splitting a job into 100 slices and distributing them to idle workstations in phase 1 to achieve parallel docking.

Together with 2-phase docking as well as slice-level parallelism, using a Mac workstation alone, on average it takes a week to complete one single slice. Therefore, even though we thoroughly utilize all the 4 Mac workstations, it can take up to 5 months to complete one single job. Currently we have 3 jobs at hand, adding up to 15 months, and we believe more jobs are coming in the future. In order to boost job execution, we plan to port idock to GPU using both CUDA and OpenCL/WebCL in both C++ and JavaScript and deploy new workstations equipped with high-end GPU chips.

\section{igrep on istar}

In addition to idock, we also hosted our igrep \citep{1138} on istar. igrep is a fast CUDA implementation of agrep algorithm for approximate nucleotide sequence matching. Its workflow is more or less identical to idock. Likewise, igrep also supports RESTful API for job query and submission.

\section*{Acknowledgement}

We would like to thank Professor John J. Irwin, the developer and maintainer of ZINC \citep{532,1178}, for granting us permission to use ZINC with three conditions:
\begin{enumerate}
\item We shall provide links to http://zinc.docking.org/substance/zincid for top hits so that users can seek for the most current purchasing information at ZINC's official web site.
\item We shall limit the number of top hits for download to 1000 ligands from a single job.
\item We shall update our ligands when ZINC data is updated so that users can benefit from the most current ligand data.
\end{enumerate}

\bibliographystyle{natbib}
\bibliography{document}

\end{document}
