\chapter{Our Proposal}

Our ultimate goal is to build a comprehensive and universal framework for computer-aided drug discovery, incorporating capabilities of binding site identification, molecular docking, virtual screening, ligand synthesis, ADMET properties prediction, and so on, and utilize such framework to address practical drug discovery problems in real life. The development of idock is just the start. There is a long way to go to fully implement all the capabilities and apply them to case studies.

\section{idock 2.0: GPU Acceleration of idock}

Even though idock 1.5 outperformed AutoDock Vina by 10x in terms of docking speed, it still required about 10 hours on average to dock 10,928 drug-like ligands against a certain protein, not to mention massive docking of millions of ligands. Virtual screening remains a time-consuming practice. Faster implementations are highly desired.

We used AMD CodeAnalyst Performance Analyzer v3.6 to detect program hotspots and observe thread behaviors. Figure \ref{idock:ThreadProfile} shows the thread profile of idock. Thread 1060 was the main thread, while the other four were workers threads spawned by the main thread and maintained by our novel thread pool. During program startup, the main thread parsed command line arguments, initialized necessary variables, parsed receptor file, and created a thread pool to precalculate the scoring function in parallel. Afterwards, it entered a loop, docking ligands one by one. The four workers threads actually carried out the creation of grid maps as well as running Monte Carlo tasks in parallel, fully occupying all the four CPU cores. Upon completion of docking a ligand, program control was returned to the main thread to write conformations to file and read the next ligand from file. From the figure, it can be concluded that the worker threads acquire most of the CPU computing resources and the precalculation of grid maps and the execution of Monte Carlo tasks constitute the program hotspots, albeit the CPU-intensive Monte Carlo tasks and the I/O-intensive file reading/writing can be possibly pipelined according to our in-house trial.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{idock/ThreadProfile.png}
\caption{idock thread profile.}
\label{idock:ThreadProfile}
\end{figure}

We propose idock 2.0, incorporating GPU acceleration with both CUDA and OpenCL, harnessing the tremendous computational power and memory bandwidth offered by modern GPUs nowadays. In \citeyear{1138} we developed a fast CUDA implementation of agrep algorithm for approximate nucleotide sequence matching \citep{1138}, demonstrating our expertise in CUDA programming. Meanwhile, we are eagerly learning OpenCL. We will first work on a CUDA version, followed by an OpenCL version.

On the hardware side, thanks to Mr. Frank Ng from ITSC (Information Technology Services Center) at Chinese University of Hong Kong, we are borrowing a GeForce GTX 680 for CUDA development and plan to purchase a Radeon HD 7970 for OpenCL development.

On the software side, performance optimization revolves around three basic strategies: 1) maximizing parallel execution, 2) maximizing memory bandwidth, and 3) maximizing instruction throughput.

Maximizing parallel execution can be achieved by exposing as much data parallelism as possible and mapping the parallelism to the hardware as efficiently as possible. In dock, we have parallelized both the precalculation of grid maps and the Monte Carlo global optimization using our novel thread pool in order to thoroughly utilize multicore CPU. We plan to port these two most time-consuming parts to the GPU, map Monte Carlo tasks directly to CUDA threads, and use NVIDIA's occupancy calculator to carefully choose the execution configuration of each kernel launch in order to maintain a high GPU utilization. Several technical difficulties exist. One difficulty is the lack of sufficient capacity of GDDR5 memory, which is merely 2GB along with a GeForce GTX 680. This restriction voids the precalculation of grid maps at a fine granularity like 0.08\AA, leading to reduced approximation accuracy and possibly a high false negative rate. Another difficulty is the efficient generation of pseudo random numbers. Although there are official libraries and third-party libraries to facilitate this purpose, it is hard to determine the number of random numbers in need in advance because the Monte Carlo algorithm is stochastic \textit{per se}.

Maximizing memory bandwidth can be achieved by minimizing data transfers between the CPU and the GPU and optimizing the access patterns to global memory and shared memory on the GPU. Since CPU-to-GPU and GPU-to-CPU data transfers have much lower bandwidth than internal GPU data transfers, we plan to accommodate as much data as possible into the GPU global memory. In idock 2.0, constant data such as structure of receptor, definition of search space, precalculation of scoring function, and configurations for the BFGS Quasi-Newton local optimizer will reside in constant cache, while grid maps, due to its huge size, will reside in global memory, and temporary variables will reside in per-thread registers.

Maximizing global memory bandwidth is of crucial importance, and its bandwidth depends largely on its access pattern. Figure \ref{GPU:AlignedSequentialGlobalMemoryAccess} shows an example of aligned and sequential global memory access and corresponding memory transactions based on compute capability. In this case, 32 threads of a warp access adjacent 4-byte words such as adjacent single precision float values or 32-bit integer values. In other words, the \textit{k}th thread accesses the \textit{k}th 4-byte word in a 128B L1 cache line, a single coalesced transaction alone will service that memory access. In idock 2.0, we will adopt this kind of aligned and sequential access pattern and re-organize array of structures into structure of arrays, e.g.  [ \{ x1, y1, z1 \}, \{ x2, y2, z2 \} ] into \{ [ x1, x2 ], [ y1, y2 ], [ z1, z2 ] \}. Such a restructuring requires rewriting almost all the relevant mathematical data structures and functions in use in idock. So far we have stepped towards this direction a little bit, finishing rewriting the template class of quaternion from the BOOST C++ library into our own lightweight version to represent the orientation of a conformation.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{GPU/AlignedSequentialGlobalMemoryAccess.png}
\caption{Aligned and sequential global memory accesses by a warp, 4-byte word per thread, and associated memory transactions based on compute capability. Source: NVIDIA.}
\label{GPU:AlignedSequentialGlobalMemoryAccess}
\end{figure}

A GK104 SMX has 64KB of on-chip memory that can be configured as 48KB of shared memory with 16KB of L1 cache, or as 16KB of shared memory with 48KB of L1 cache. Since threads within a thread block run their Monte Carlo tasks independently and seldom communicate with one another, we decide to allocate 48KB of the 64KB on-chip memory to L1 cache.

Maximizing instruction throughput can be achieved by using single precision floating point instead of double precision and using intrinsics instead of regular functions. This strategy suggests trading precision for speed as long as the final result is not affected. Since most contemporary GPU chips supply with an astonishingly high throughput for single precision operations at TFLOP level but a relatively low throughput for double precision operations, we prefer the former. In order to make sure the precision loss must not affect the end result too much, we did an in-house trial, demoting double to float in idock, only to find that the predicted conformation and free energy were exactly identical as in the case of double precision given the same random seed for initializing the pseudo random number generator. This experiment concluded idock to be insensitive to precision switch and it is thus safe to utilize single precision operations as well as native intrinsics in idock 2.0.

\section{istar: SaaS Platform for idock}

Having released and advertised idock, we kept receiving constant docking requirements from our colleges and collaborators. They are mostly biochemists and pharmacists, outsourcing the docking research to us after discovering potential biological targets for certain diseases of therapeutic interest. All of a sudden, we had to grab the protein structure, do format conversion, define search space, set up docking parameters, and keep running idock for months. Occasionally we also had to restart virtual screening in case the idock process got killed by others or by exceptions. Tedious enough, all the above work was done manually, resulting in very low research productivity.

We propose istar, a SaaS (Software as a Service) platform for idock. The goal of istar is to automate the virtual screening process. Without tedious software installation, users, especially computational chemists, can submit jobs on the fly in either of three ways: 1) browsing our web site (Figure \ref{istar:Website}), 2) programming against our RESTful API (Figure \ref{istar:RESTfulAPI}), or 3) sending emails conforming to our specifications (Figure \ref{istar:Mail}).

\begin{figure}
\centering
\includegraphics[width=\linewidth]{istar/Website.png}
\caption{istar web site.}
\label{istar:Website}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{istar/RESTfulAPI.png}
\caption{istar RESTful API.}
\label{istar:RESTfulAPI}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{istar/Mail.png}
\caption{istar mail specification for job submission.}
\label{istar:Mail}
\end{figure}

Figure \ref{istar:Architecture} shows the architecture of istar. On the client side, we seamlessly combine both the Twitter Bootstrap and the HTML5 Boilerplate into a HTML5- and CSS3-powered web site. We use the Modernizr JavaScript library to detect HTML5 and CSS3 features in order to maintain backward compatibility in older browsers. We adopt the \textit{de facto} jQuery JavaScript library to simplify HTML document traversing, event handling, animating, and Ajax interactions. We test the user interface on Google Chrome 19+, Mozilla Firefox 12+, Microsoft Internet Explorer 9+ and Apple Safari 5+.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{istar/Architecture.png}
\caption{istar architecture.}
\label{istar:Architecture}
\end{figure}

On the server side, we build the web server in the same JavaScript language using the well-known asynchronous event-driven node.js on top of the express server, forking multiple worker processes to accept simultaneous HTTP connections. During startup, the master process loads and parses 9 molecular properties of 12 million ligands (Figure \ref{istar:LigandProperties}), and awaits queries from worker processes through inter-process message passing. Upon receival of a query from a worker process, the master process performs an in-memory table scan and calculates the number of ligands satisfying a given combination of filtering conditions on those 9 molecular properties. The ligand count was signaled to the query-invoking worker process and then forwarded to the client side. The entire round trip costs approximately one second, rendering the feasibility of real-time ligand counting queries for users to estimate in advance how many ligands will be docked. We expose the functionalities of job submission, job query, and ligand counting as RESTful API for others to program against. We also set up a gmail account and develop a mail crawler using node.js on top of Context.IO, a RESTful API for manipulating mailboxes, messages and attachments, to retrieve new jobs directly from emails from time to time, temporarily hourly. We employ the forever utility for both the web server and the mail crawler to automatically restart processes in case of unhandled exceptions, ensuring failover and high availability. We are meanwhile considering prepending the scalable asynchronous event-driven nginx as a front-end HTTP server for a rich set of features, like bandwidth throttling, static file serving, reverse proxy for caching, load balancing, URL rewriting, gzip compression, TLS/SSL support, IP-based geolocation, etc.

\begin{figure}
\centering
\subfloat
{
  \includegraphics[width=0.315\linewidth]{istar/MWT.pdf}
}
\subfloat
{
  \includegraphics[width=0.315\linewidth]{istar/LogP.pdf}
}
\subfloat
{
  \includegraphics[width=0.315\linewidth]{istar/Desolv_apolar.pdf}
}
\\
\subfloat
{
  \includegraphics[width=0.315\linewidth]{istar/Desolv_polar.pdf}
}
\subfloat
{
  \includegraphics[width=0.315\linewidth]{istar/HBD.pdf}
}
\subfloat
{
  \includegraphics[width=0.315\linewidth]{istar/HBA.pdf}
}
\\
\subfloat
{
  \includegraphics[width=0.315\linewidth]{istar/tPSA.pdf}
}
\subfloat
{
  \includegraphics[width=0.315\linewidth]{istar/Charge.pdf}
}
\subfloat
{
  \includegraphics[width=0.315\linewidth]{istar/NRB.pdf}
}
\caption{Distributions of 9 molecular properties of the 12,171,187 clean ligands.}
\label{istar:LigandProperties}
\end{figure}

On the database side, we adopt MongoDB, a scalable, high-performance, open source NoSQL database. MongoDB features document store in JSON style, making it particularly suitable for applications requiring flexible document attributes like our istar. MongoDB is written in C++, making it easy to manipulate database operations from idock, which is also written in C++. By using a 10gen-supported native MongoDB driver for node.js, we are able to save jobs from both the web server and the mail crawler. Originally we stored the 9 molecular properties in MongoDB, but our in-house trial showed that it took about 9 seconds to complete a ligand counting query even in the presence of index and even having the data stored in a RAM disk. Disappointed by the poor performance of MongoDB given 12 million documents, we decided to abandon this design and let the master process hold the data in memory throughout its lifecycle.

On the network file system side, we have collected 12,171,187 clean ligands at pH 7 in mol2 format from the fruitful ZINC database \citep{532} with explicit permission of its major developer and maintainer, Prof. John J. Irwin at University of California, San Francisco. We have converted all the 12 million ligands into PDBQT format in batch, and combined the 12 million individual files and integrated their 9 molecular properties into one single file as huge as 50GB for subsequent parallel docking. The format conversion alone already required 2 months' time. We have also recorded and serialized the offset of each ligand within that big file for fast random seeking. Originally the yuck-compound-free ligands are organized into 98 slices, with each slice containing approximately 125,000 clean ligands, among which about 75,000 (60\%) have a molecular weight of at least 350g/mol (Figure \ref{istar:gt350}), the minimum weight requirement that a modern drug should bear. The entire network file system is shared by all the back-end workstations which actually carry out the massive docking experiments.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{istar/gt350.pdf}
\caption{Distributions of ligands whose molecular weight is above 350g/mol of the 98 slices.}
\label{istar:gt350}
\end{figure}

On the workstation side, we develop a customized version of idock in C++11, adding necessary code to iteratively fetch a pending job slice from MongoDB, reuse receptor and grid maps if possible, perform 2-phase virtual screening, report progress to MongoDB from time to time, and send email notification upon job completion. From our department, we exclusively reserve 4 workstations, equipped with Intel Core i5-2400 CPU @ 3.10GHz and 4GB DDR3 SDRAM and running Mac OS X Lion 10.7.4 Build 11E53. Using a workstation, on average it takes a week to screen one single slice. Therefore, even though we thoroughly utilize all the 4 workstations, it can take up to 20 weeks, i.e. 5 months, to complete one single job. Currently we have three jobs at hand, adding up to 15 months, and we believe more jobs are coming in the future. In order to boost job execution and expect a speed up of at least an order of magnitude, we plan to port idock to GPU using both CUDA and OpenCL/WebCL in both C++ and JavaScript and deploy new workstations equipped with high-end GPU chips. We are collaborating with Mr. Frank Ng at ITSC on purchasing a HP s6500 4U Chassis server with 8 NVIDIA Tesla M2090 6GB.

Figure \ref{istar:Website} shows istar web site. Job progress will be displayed in the upper section, while new jobs can be submitted in the lower section. A new job typically consists of several compulsory fields as well as several optional fields. Compulsory fields include a receptor in PDBQT format, a search space defined by a cubic box, a short job description, and an email to receive completion notification. Optional fields include 9 ligand filtering conditions, like molecular weight, partition coefficient xlogP, apolar desolvation, polar desolvation, number of hydrogen bond donors, number of hydrogen bond acceptors, topological polar surface area tPSA, net charge, and number of rotatable bonds. We set up default values for optional fields, and only the ZINC ligands satisfying all the 9 filtering conditions will be screened by idock.

istar features slice-level parallelism. Generally speaking, multiple workstations can compete for either jobs or slices, with the former known as job-level parallelism and the latter as slice-level parallelism (Figure \ref{istar:2LevelParallelism}). Job-level parallelism is very straightforward to implement and can ensure high utilization of computational power when the number of jobs exceeds the number of workstations. Nevertheless, when the number of workstations exceeds the number of jobs, which is usually the case in practice during the initial stage of istar, slice-level parallelism can better utilize computational power by subdividing a job into slices which are then distributed to workstations. Slice-level parallelism is, in contrast, difficult to implement on both the database side and the workstation side. The technical hurdle becomes even more apparent when results from multiple workstations must be properly combined to produce a final result and progresses from multiple workstations must be properly combined too to compute an overall progress.

\begin{figure}
\centering
\subfloat[Job-level parallelism.]
{
  \label{istar:JobLevelParallelism}
  \includegraphics[width=\linewidth]{istar/JobLevelParallelism.png}
}
\\
\subfloat[Slice-level parallelism.]
{
  \label{istar:SliceLevelParallelism}
  \includegraphics[width=\linewidth]{istar/SliceLevelParallelism.png}
}
\caption{istar 2-level parallelism.}
\label{istar:2LevelParallelism}
\end{figure}

istar features 2-phase docking, due to as many as 12 million ligands to screen. In phase 1, idock performs coarse but fast virtual screening without writing any conformations to file, aiming to quickly shortlist a few thousand candidate compounds. In phase 2, idock performs fine but slow virtual screening with a significantly larger number of Monte Carlo tasks per ligand, writing as many conformations to file as possible and aiming to refine the predicted free energy as well as predicted conformation of candidate compounds. Such a 2-phase docking methodology can remarkably reduce job execution time while avoiding the risk of filtering out potentially promising compounds, controlling the false negative rate at an acceptable level.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{istar/2PhaseDocking.png}
\caption{istar 2-phase docking.}
\label{istar:2PhaseDocking}
\end{figure}

As for software availability, similar to idock, istar is also hosted at GitHub for version control and freely available at https://github.com/HongjianLi/istar. Its source code is licensed under the Apache License 2.0 while its documentation is licensed under CC BY 3.0. A live demo will be available at http://istar.cse.cuhk.edu.hk when we finish implementing istar.

istar serves as our foundation for SaaS. With istar at hand, basically we can expose any of our software as a service. Once we manage to host idock at istar and accumulate precious experience, it will be quite easy for us to host other tools too, such as our igrow for computational synthesis of potent ligands.

\section{igrow: Ligand Synthesis for idock}

Synthesis and docking are closely related. Figure \ref{igrow:SynthesisAndDocking} shows their recursive relationship. Tiny fragments are combined in accordance to chemistry rules to synthesize larger ligands, which are then docked to a receptor to evaluate their fitness. The best ligands enter the next iteration to synthesize other ligands, which are again evaluated by docking, until promising ligands are discovered. AutoGrow \citep{466} is a representative ligand synthesis tool that makes use of GA (Genetic Algorithm) to automate this recursive procedure.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{igrow/SynthesisAndDocking.png}
\caption{Iteratively synthesizing novel ligands from fragments followed by docking to a receptor.}
\label{igrow:SynthesisAndDocking}
\end{figure}

In 2011, inspired by AutoGrow, we developed SmartGrow, addressing several problems that AutoGrow suffers from. SmartGrow not only inherited the mutation and crossover operators from AutoGrow, but also invented the merge and split operators. It implemented Lipinski's \textit{Rule of Five} \citep{168} to ensure drug likeness. Its robust parser correctly processed two-letter chemical elements like Cl (chlorine) and Br (bromine), and meanwhile added additional support for phosphorus. Results showed that SmartGrow displayed comparable performance in terms of predicted free energy but outperformed AutoGrow by 30\% in terms of execution time on average across 18 test cases. Besides, ligands generated by SmartGrow resulted in 100g/mol lower molecular weights than AutoGrow and never exceeded 500g/mol so that they can be absorbed by human body.

However, we wrote SmartGrow in a hurry and did not follow a formal software engineering approach. Later on when we conducted a benchmark on SmartGrow, we discovered a great many of exceptional bugs that were rather hard to track due to the messy code structure and lack of comments. Regretfully, we decided to abandon the buggy SmartGrow.

We propose igrow as a successor of SmartGrow. Instead of porting existing code from SmartGrow and fixing bugs line by line, we rewrote igrow from scratch in a systematic manner. Figure \ref{igrow:Flowchart} shows the flowchart of our current implementation of igrow. The program design is so flexible that it reserves room for adaptation to new chemical constraints. So far we have implemented the selection and mutation operators. The crossover operator is yet to implement.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{igrow/Flowchart.pdf}
\caption{igrow flowchart.}
\label{igrow:Flowchart}
\end{figure}

Compared with AutoGrow and SmartGrow, our igrow features a plenty of advantageous innovations.
igrow supports more types of chemical synthesis such as halogen replacement and branch replacement in addition to hydrogen replacement.
igrow digests ligands and fragments in pdbqt format, saving the effort of frequently calling external python script.
igrow invents its own thread pool in order to reuse threads and maintain a high CPU utilization throughout the entire synhsizing procedure. The thread pool parallelizes the creation of mutants and children in each generation.
igrow utilizes flyweight pattern for caching fragments and dynamic pointer vector for caching and sorting ligands.
igrow traces the sources of generated ligands and dumps the statistics in csv format so that users can easily get to know how the ligands are synthesized from the initial ligand and fragments.
Allowed users to specify the ranges of several chemical properties such as molecular weight.
Allowed users to specify the number of failures of GA operations as a stopping criterion.
Supported direct PDBQT manipulation without file format conversion.
Used dynamic pointer vector to cache ligands
igrow actually borrows several great ideas from idock. For example, igrow utilizes our novel thread pool in order to parallelize the mutation and crossover operators and reuse threads throughout the entire synthesis procedure. igrow estimates the capacity of every vector structure and intensively utilizes rvalue reference, a new feature in the C++11 standard, to avoid frequent memory reallocation.

Clearly, there are two major problems with our current design. Problem one is the chemical infeasibility of ligands synthesized. This is a functional problem. Problem two is the high cost of invoking external idock in every generation. This is a computational problem.

Functionally speaking, ligands synthesized by our mutation and crossover operators, albeit chemically valid, might not be chemically synthesizable, simply because the two operators are somewhat arbitrary and do not follow any known or general chemical reactions. In other words, that we can computationally synthesize novel ligands does not imply we can chemically synthesize them in reality. We may produce very fancy ligands at will, but all are just in computer simulations. To address the problem, we, inspired by AutoClickChem \citep{1051}, plan to incorporate click chemistry into igrow to make sure every step of synthesis does follow some kind of well-known chemical reaction.

Computationally speaking, igrow relies on idock as its external docking engine and thus has to invoke idock after synthesizing new ligands every generation, repeatedly reading identical receptor and creating identical grid maps. Therefore the best way to go is to directly integrate igrow into idock. We will try to integrate igrow into idock to realize grid map caching as well as partial docking.
 
As for software availability, igrow is free and open source under Apache License 2.0. It is written in C++ and available at https://github.com/HongjianLi/igrow. Precompiled executables for 32-bit and 64-bit Linux, Windows, Mac OS X, FreeBSD and Solaris are provided. Use cases and API documentations are also provided.

\section{ADMET Property Prediction}

It was estimated that 40\% to 60\% of new chemical entity (NCE) failures can be attributed to poor ADMET (absorption, distribution, metabolism, excretion, and toxicity) profiles.

PKKB (PharmacoKinetics Knowledge Base) \citep{1133} provides the most extensive collection of freely available data for ADMET properties up to date. PKKB integrates 31,412 experimental or predicted values for 1,685 drug and drug-like molecules from published literature with available experimental ADMET properties including partition coefficient (logP), solubility (logS), intestinal absorption, Caco-2 permeability, human bioavailability, plasma protein binding, volume of distribution, distribution of blood, half-time, excretion, urinary excretion, clearance, toxicity, etc. (Table \ref{PKKB:Properties})

\begin{table}
\centering
\begin{tabular*}
{\linewidth}
{@{\extracolsep{\fill}}rlr}
\toprule
No. & Property & Measurements \\
\midrule
\multicolumn{3}{l}{\textbf{Molecular properties}}\\
1 & Molecular weight & 1,684 \\
2 & logP (experiment) & 1,019 \\
3 & logP (predicted, AB/logP v2.0) & 1,625 \\
4 & Pka & 638 \\
5 & logD (pH=7,predicted) & 1,625 \\
6 & Solubility (experiment) & 800 \\
7 & logS (predicted, ACD/Labs)(pH=7) & 1,614 \\
8 & logSw (predicted, AB/LogSw 2.0) & 1,625 \\
9 & Sw (mg/ml) (predicted, ACD/Labs) & 1,613 \\
10 & Sw (predicted) & 1,625 \\
11 & No. of hbond donors & 1,625 \\
12 & No. of hbond acceptors & 1,625 \\
13 & No. of rotatable bonds & 1,625 \\
14 & TPSA & 1,625 \\
\noalign{\smallskip\smallskip}
\multicolumn{3}{l}{\textbf{Pharmacology}}\\
15 & Status & 1,372 \\
16 & Administration & 501 \\
17 & Pharmacology & 1,543 \\
\noalign{\smallskip\smallskip}
\multicolumn{3}{l}{\textbf{Absorption}}\\
18 & Intestinal absorption & 679 \\
19 & Absorption (description) & 699 \\
20 & Caco-2 & 64 \\
21 & Human bioavailability & 992 \\
\noalign{\smallskip\smallskip}
\multicolumn{3}{l}{\textbf{Distribution}}\\
22 & Plasma protein binding & 1058 \\
23 & Volume of distribution(Vd) & 646 \\
24 & D-blood & 66 \\
\noalign{\smallskip\smallskip}
\multicolumn{3}{l}{\textbf{Metabolism}}\\
25 & Metabolism & 1,111 \\
26 & Half-time & 1,116 \\
\noalign{\smallskip\smallskip}
\multicolumn{3}{l}{\textbf{Excretion}}\\
27 & Excretion & 855 \\
28 & Urinary excretion & 281 \\
29 & Clearance & 410 \\
\noalign{\smallskip\smallskip}
\multicolumn{3}{l}{\textbf{Toxicity}}\\
30 & Toxicity & 873 \\
31 & LD50 (rat) & 219 \\
32 & LD50 (mouse) & 243 \\
\bottomrule
\end{tabular*}
\caption{PKKB property measurements. Data obtained from PKKB web site.}
\label{PKKB:Properties}
\end{table}

Figure \ref{PKKB:Schema}, reprinted from \citep{1133}, shows the PKKB schema. Raw data from literature was first converted into proper formats and then integrated into a central ADMET database. A web site is provided for interactive search of ADMET properties given either structure or descriptive text of compounds of interest.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{PKKB/Schema.jpg}
\caption{PKKB schema. Figure reprinted from \citep{1133}.}
\label{PKKB:Schema}
\end{figure}

Thanks to the availability of reliable experimental data and basic structural information from PKKB enables, \textit{in silico} ADMET modeling becomes viable. One can predict ADMET properties from chemical structures for a huge number of compounds without actually synthesizing or assaying them.

We propose to make use of advanced machine learning techniques to train a regression model (Figure \ref{PKKB:Regression}), be it linear or non-linear. Once the model is constructed, it is possible to predict ADMET properties of any given compounds. Coupled with conformations and free energy predicted by our tool idock, we can better evaluate a compound from a wide variety of perspectives, leading to a higher drug discovery success rate. Moreover, such a regression model can be integrated into igrow to guide ligand synthesis, which can then be further integrated into idock to guide ligand filtering and eventually into istar to compose a comprehensive framework for structure-based drug discovery.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{PKKB/Regression.png}
\caption{Our proposed regression model for PKKB.}
\label{PKKB:Regression}
\end{figure}

\chapterend
